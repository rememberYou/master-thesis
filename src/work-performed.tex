
\chapter{Work Performed}
\label{chap:work:performed}

This chapter is dedicated to the solutions provided by this Master's
thesis. These solutions include:
\begin{itemize}
\item \textbf{Improving the accuracy of the Word2Vec model}: using a user-defined
depth to extract the child and parent nodes of a current node for each walk. In
addition, the centralization of the root node in the extracted walks
maximizes the number of training samples containing this root node and therefore
generates better quality embeddings.
\item \textbf{A BERT implementation}: builds its vocabulary only based on
special tokens and single extracted nodes, not allowing WordPiece splitting in
the tokenization of nodes. Finally, this version of BERT only considers
the MLM as a pre-training step, with training that tends to find a trade-off
between its time and the accuracy that this model will generate.
\item \textbf{A FastText implementation}: unlike its original version, its
implementation does not allow splitting $n$-grams on a defined minimum and maximum
length, but only according to a splitting function provided by a user. In
addition, the reimplementation of the Cython $n$-grams computation function in
Python to facilitate the use of \texttt{pyRDF2Vec} on Google
Colaboratory\footnote{Website that allows to run code on Google's cloud
servers.}.
\item \textbf{WideSampler}: sampling strategy that maximizes the extraction of
  shared features between entities.
\item \textbf{SplitWalker}: walking strategy that extracts walks according to a
  splitting function.
\item \textbf{A better architecture}: the \texttt{pyRDF2Vec} library now allows
to easily add walking strategies, sampling strategies, and embedding techniques
by reimplementing only a few functions.
\end{itemize}

Finally, this Master's thesis implicitly proposes some research for future work
related to BERT. Among these, the injection of \texttt{(subject, object)}
2-tuples in BERT instead of two walks. Such an injection would allow BERT to
focus to predict predicates instead of seeing correlations between two different
walks.

\section{Improving the Accuracy of the Word2Vec Model}
\label{sec:work:performed:word2vec}

To better understand how it is possible to improve the model's accuracy
generated by Word2Vec, it is helpful to consider the initial problem with the
walk extraction. With RDF2Vec, the transformation for a $n$-tuple without any walks
limitation and concerning {\footnotesize\texttt{"URL\#Alice"}} as the root node
can be achieved with each walking strategy as follows:
\begin{table}[!ht]
  \centering
  \begin{threeparttable}[t]
    \centering
    \resizebox{\textwidth}{!}{%
      \begin{tabular}{cll}
        \toprule
        \textbf{Walking Strategy} & \textbf{Initial/Transformed $\mathbf{n}$-tuple} \\
        \midrule
        \multirow{2}{*}{Anonymous Walk} & {\footnotesize\texttt{("URL\#Alice", "URL\#knows","URL\#Bob")}} \\
        & {\footnotesize\texttt{("URL\#Alice", "1", "2")}} \\
        \midrule
        \multirow{2}{*}{HALK} & {\footnotesize\texttt{("URL\#Alice", "URL\#knows", "URL\#Bob")}, \texttt{("URL\#Alice", "URL\#loves", "URL\#Bob")},} $\dotsc$ \\
        & {\footnotesize\texttt{("URL\#Alice", "URL\#knows","URL\#Bob")}}\tnote{\textcolor{blueLink}{1}} \\
        \midrule
        \multirow{2}{*}{N-Gram} & {\footnotesize\texttt{("URL\#Alice", "URL\#knows", "URL\#Bob")}, \texttt{("URL\#Alice", "URL\#loves", "URL\#Dean")}} \\
        & {\footnotesize\texttt{("URL\#Alice", "URL\#knows", "0")}, \texttt{("URL\#Alice", "URL\#loves", "1")}}\tnote{\textcolor{blueLink}{2}} \\
        \midrule
        \multirow{2}{*}{Walklets} & {\footnotesize\texttt{("URL\#Alice", "URL\#knows","URL\#Bob")}} \\
        & {\footnotesize\texttt{("URL\#Alice", "URL\#knows")}, \texttt{("URL\#Alice", "URL\#Bob")}} \\
        \midrule
        \multirow{2}{*}{Weisfeiler-Lehman} & {\footnotesize\texttt{("URL\#Alice", "URL\#knows","URL\#Bob")}} \\
        & {\footnotesize\texttt{("URL\#Alice", "URL\#knows", "URL\#Bob")}, \texttt{("URL\#Alice", "URL\#knows", "URL\#Bob-URL\#knows")}}\tnote{\textcolor{blueLink}{3}} \\
        \bottomrule
      \end{tabular}
    }%
    \begin{tablenotes}
    \item[1]
      Assuming a minor threshold frequency and an infrequent
      {\footnotesize\texttt{"URL\#loves"}} hop compared to \\ other hops.
    \item[2] Assuming at least one gram to relabel. If grams $> 2$, every object
      names will remain \\ identical for this example.
    \item[3] Assuming a Weisfeiler Lehman iteration of 1.
    \end{tablenotes}
  \end{threeparttable}%
  \caption{Example of $n$-tuple Transformation for Type 2 Walking Strategies.}
  \label{tab:walking:strategies}
\end{table}

The walking strategies in Table \ref{tab:walking:strategies} already give good
results. However, the {\footnotesize\texttt{"URL\#Alice"}} root node is always
positioned at the beginning of each walk, reducing the richness of these walks
for embedding techniques like Word2Vec, which works with window size. As the
solicitation of nodes placed in the middle of a walk is higher due to selecting
context words, including the root node as close as possible to the middle for
each walk could generate better quality embeddings. Indeed, much more training
sampling would contain this root node.

In addition to this positioning issue of the root node, \texttt{type 1} walking
strategies have the main disadvantage of continuously extracting child nodes of
a root node. In other words, the embedding techniques never know the root node's
parents. An alternative would be to extract the whole child and parent nodes of
a root node. Such extraction maximizes the extracted walk information and,
therefore, improves a model's accuracy after being trained with an embedding
technique. However, this alternative is not suitable when extracted walks are
limited, mainly used to process large KGs. Furthermore, when it comes to
positioning, the extraction of parent nodes from a root node also implies a
wrong positioning. Specifically, this root node would be this time placing as
the last node of walks, preceded by a sequence of predicates and objects, which
is not desirable.

Based on these constraints, this Master's thesis proposes to apply a
user-defined depth to extract the child and parent nodes of a current node for
each walk. Assuming that the root node has parent nodes, each extracted walk
will have a better position for this root node. Indeed, each root node will be
preceded by a part of its parent nodes and succeeded by a part of its child
nodes.

In Table \ref{tab:window:size}, assuming the sentence ``I will always remember
her'' and considering the word ``I'' as the root node of a walk, the latter is
only included twice in the context words. However, suppose now this word is
positioned in the center instead of the word "always". In that case, its
frequency of occurrence can be doubled, i.e., from two to four times for this
example. As a result, the embeddings generated for the different root nodes are
of better quality by the number of occurrence of root nodes and by the context
provided.

\section{BERT Implementation}
\label{sec:bert:implementation}

The recommended library to implement BERT is
\texttt{huggingface/transformers}\footnote{\url{https://huggingface.co/transformers/}}
which provides many pre-trained
models\footnote{\url{https://huggingface.co/models}} and essential functions to
create a model. As no pre-training model exists for BERT with KGs, it is
necessary to create this model from scratch, which requires three main steps:
\begin{enumerate}
\item \textbf{Build the vocabulary}: based on the nodes, including one line per
  special token and one line per unique node, as part of a KG.
\item \textbf{Fits the BERT model}: based on the corpus of walks provided, including three main goals:
  \begin{multicols}{2}
    \begin{enumerate}
    \item \textbf{Node tokenization}: in such a way that special tokens are
      inserted on both sides of the nodes, taking care not to split them. Which unlike
      words, splitting a URI is not desired.
    \item \textbf{Pre-training}: only done with MLM, as the NSP pre-training
      task is not helpful since the walks do not share any continuity.
    \item \textbf{Training}: is done by providing a training set of formatted
      walks, a data collator (e.g., \texttt{DataCollatorForLanguageModeling}), and the
      training parameters. The walk formatting is necessary to ensure added
      padding, truncating walks that are too long ($\geq$ 512 characters).
    \end{enumerate}
  \end{multicols}
\item \textbf{Transforms the provided entities into embeddings}: returns entity embeddings.
\end{enumerate}

\begin{lstlisting}[caption=Creation of the Walk Data Set.,label=bert:walk:dataset]
class WalkDataset(Dataset):
  def __init__(self, corpus, tokenizer):
    self.walks = [
      tokenizer(
        " ".join(walk), padding=True, truncation=True, max_length=512
      )
      for walk in corpus
    ]

  def __len__(self):
    return len(self.walks)

  def __getitem__(self, i):
    return self.walks[i]
\end{lstlisting}

In Algorithm \ref{bert:walk:dataset}, creating the walks data set for BERT
is done by creating a dedicated class. Within this class, tokenization is necessary.
Each walk is truncated to 512 characters, followed by padding to handle
walks of the same size. Finally, it is also useful to implement the
Dunder\footnote{Also called \emph{magic} methods.}  \texttt{\_\_len\_\_} and
\texttt{\_\_getitem\_\_} methods to define the size of a sample of training data
and for the fetching of this sample.

\begin{lstlisting}[caption=Creation of Node Vocabulary.,label=bert:node:vocabulary]
def _build_vocabulary(self, nodes, is_update = False):
  with open(self.vocab_filename, "w") as f:
    if not is_update:
      for token in ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]:
        f.write(f"{token}\n")
        self._vocabulary_size += 1
    for node in nodes:
      f.write(f"{node}\n")
      self._vocabulary_size += 1
\end{lstlisting}

In Algorithm \ref{bert:node:vocabulary}, each walk node is stored in a file with
special tokens at the beginning of the file, namely: \texttt{[PAD]},
\texttt{[UNK]}, \texttt{[CLS]}, \texttt{[SEP]}, and \texttt{[MASK]}. Finally, a
boolean is also sent to update or not an already existing vocabulary and avoid
the complete re-training of the model.

\begin{lstlisting}[caption=Fitting the BERT Model According to the Provided Walks.,label=bert:fit]
def fit(self, walks, is_update = False):
  walks = [walk for entity_walks in walks for walk in entity_walks]
  nodes = list({node for walk in walks for node in walk})
  self._build_vocabulary(nodes, is_update)
  self.tokenizer = BertTokenizer(
    vocab_file=self.vocab_filename,
    do_lower_case=False,
    never_split=nodes,
  )
  self.model_ = BertForMaskedLM(
    BertConfig(
      vocab_size=self._vocabulary_size,
      max_position_embeddings=512,
      type_vocab_size=1,
      )
   )
   Trainer(
     model=self.model_,
     args=self.training_args,
     data_collator=DataCollatorForLanguageModeling(
       tokenizer=self.tokenizer
     ),
     train_dataset=WalkDataset(walks, self.tokenizer),
   ).train()
 \end{lstlisting}

In Algorithm \ref{bert:fit}, the training of the BERT model consists of
retrieving each unique node from the provided walks, tokenizing these walks,
choosing a suitable hyperparameter config, and running the training based on a
set of walks data.

BERT needs to be trained on a large enough corpus of walks to provide good
results. Having such a corpus is not always possible, depending on the size of
some KGs. In addition, hyperparameters for training BERT impact the model's
accuracy and training time. This training time can take hours, days, weeks, if
not more. Therefore it is necessary to improve as much as possible the quantity
and quality of the walks corpus with RDF2Vec to reduce this training time.

\begin{lstlisting}[caption=Getting the Entities' Embeddings with BERT.,label=bert:embeddings]
def transform(self, entities):
  check_is_fitted(self, ["model_"])
  return [
    self.model_.bert.embeddings.word_embeddings.weight[
      self.tokenizer(entity)["input_ids"][1]
    ]
    .cpu()
    .detach()
    .numpy()
    for entity in entities
  ]
\end{lstlisting}

In Algorithm \ref{bert:embeddings}, the retrieving of embeddings for the
requested entities, i.e., the root notes, consists of recovering the generated
embeddings and ensuring their good formatting.

These methods are sufficient for a classical implementation of BERT. All the
subtlety of generating a correct model is characterized by the extraction and
injection of walks and the values of chosen hyperparameters.

\section{FastText Implementation}
\label{sec:fasttext:implementation}

To implement FastText, the
\texttt{gensim}\footnote{\url{https://github.com/RaRe-Technologies/gensim}}
library is recommended to be used. However, \texttt{pyRDF2Vec} had to
reimplement much of the code in order:
\begin{itemize}
\item \textbf{To remove the \texttt{min\_n} and \texttt{max\_n} parameters for $n$-grams
    splitting}: the \texttt{object} nodes in \texttt{pyRDF2Vec} are encoded in MD5
  to reduce their storage in RAM. Therefore, splitting them into $n$-grams is pointless.
\item \textbf{To allow a user to compute $n$-grams for walks only by separating} (default
  split by their symbols) \textbf{the URIs of subjects and predicates}: a user
  will likely want to provide an alternative splitting strategy for computing entity
  $n$-grams on the KG. If this is the case, \texttt{pyRDF2Vec} allows a user
  to implement this function that FastText will use.
\item \textbf{To avoid dependency on Cython}: Cython is a programming language between
Python and C. Its main interest is to obtain performances of calculation time
similar to C for specific functions in Python by reimplementing them in
Cython. The gensim library uses Cython for the calculation of n-grams
hashes. However, \texttt{pyRDF2Vec} has chosen to reimplement this function in
Python to facilitate its use on Google Colaboratory.
\end{itemize}

\begin{lstlisting}[caption=Reimplementation of the Hash Calculation Functions in
  \texttt{gensim}.,label=fasttext:hash]
  def compute_ngrams_bytes(entity):
    if "http" in entity:
      ngrams = " ".join(re.split("[#]", entity)).split()
      return [str.encode(ngram) for ngram in ngrams]
    return [str.encode(entity)]

    def ft_hash_bytes(self, bytez: bytes) -> int:
      h = 2166136261
      for b in bytez:
        h = h ^ b
        h = h * 16777619
      return h

    def ft_ngram_hashes(entity, num_buckets = 2000000):
      encoded_ngrams = func_computing_ngrams(entity)
      hashes = [ft_hash_bytes(n) % num_buckets for n in encoded_ngrams]
      return hashes

    def recalc_char_ngram_buckets(bucket, buckets_word, index_to_key) -> None:
      if bucket == 0:
        buckets_word = [np.array([], dtype=np.uint32)] * len(index_to_key)
        return
      self.buckets_word = [None] * len(index_to_key)
      for i, word in enumerate(index_to_key):
        buckets_word[i] = np.array(ft_ngram_hashes(word, 0, 0, self.bucket), dtype=np.uint32)
\end{lstlisting}

In Algorithm \ref{fasttext:hash}, the functions present in \texttt{gensim} are
reimplemented in such a way to include the splitting function of
\texttt{pyRDF2Vec}. Added to that this reimplementation does not consider the
use of Cython anymore.

\begin{lstlisting}[caption=Fitting the FastText Model According to the Provided Walks.,label=fasttext:fit]
def fit(self, walks, is_update = False):
  corpus = [walk for entity_walks in walks for walk in entity_walks]
  self._model.build_vocab(corpus, update=is_update)
  self._model.train(
    corpus,
    total_examples=self._model.corpus_count,
    epochs=self._model.epochs,
  )
  return self
\end{lstlisting}

In Algorithm \ref{fasttext:fit}, training with FastText consists of extracting
each node from each walk, building the vocabulary, and training the model based
on the corpus of walks.

\begin{lstlisting}[caption=Getting the Entity Embeddings with FastText.,label=fasttext:transform]
def transform(self, entities):
  if not all([entity in self._model.wv for entity in entities]):
    raise ValueError(
      "The entities must have been provided to fit() first "
      "before they can be transformed into a numerical vector."
    )
  return [self._model.wv.get_vector(entity) for entity in entities]
\end{lstlisting}

In Algorithm \ref{fasttext:transform}, even though FastText can generate entity
embeddings that it has not learned, \texttt{pyRDF2Vec} throws an exception
instead of avoiding any unpleasant surprises from the model’s accuracy.

\section{SplitWalker}
\label{sec:split:walker}

Based on the idea of FastText, but directly applied to the extraction of walks,
this Master's thesis proposes \texttt{SplitWalker} as a new \texttt{type
2}. Specifically, this strategy splits the vertices of the random walks for a
based entity. To achieve this, each vertex, except the root node, is split
according to symbols, capitalization, and numbers by removing any duplication.

\begin{table}[!ht]
  \centering
  \resizebox{\textwidth}{!}{%
    \begin{tabular}{cll}
      \toprule
      & \textbf{Initial Node} & \textbf{Node After Splitting} \\
      \midrule
      \multirow{4}{*}{\footnotesize Walk 1} & \footnotesize\texttt{http://dl-learner.org/carcinogenesis\#d19} & \footnotesize\texttt{http://dl-learner.org/carcinogenesis\#d19} \\
      & \multirow{2}{*}{\footnotesize\texttt{http://dl-learner.org/carcinogenesis\#hasBond}} & \footnotesize\texttt{has} \\
      & & \footnotesize\texttt{bond} \\
      & \footnotesize\texttt{http://dl-learner.org/carcinogenesis\#bond3209} &\footnotesize\texttt{3209} \\
      \midrule
      \multirow{3}{*}{\footnotesize Walk 2} & \footnotesize\texttt{http://dl-learner.org/carcinogenesis\#d19} & \footnotesize\texttt{http://dl-learner.org/carcinogenesis\#d19} \\
      & \footnotesize\texttt{http://www.w3.org/1999/02/22-rdf-syntax-ns\#type} & \footnotesize\texttt{type} \\
      & \footnotesize\texttt{http://dl-learner.org/carcinogenesis\#Compound} & \footnotesize\texttt{compound} \\
      \bottomrule
    \end{tabular}
  }%
  \caption{Example of Use of \texttt{SplitWalker}.}
  \label{work:performed:splitwalker}
\end{table}

In Table \ref{work:performed:splitwalker}, two walks are transformed by
\texttt{SplitWalker}. Both keep their root node intact. However, the first walk
has the \texttt{.../hasBond} split by the letter \texttt{B} into two nodes:
\texttt{has} and \texttt{bond}. In addition, its third node
\texttt{.../bond3209} is also split into two nodes: \texttt{bond} and
\texttt{3209}, but since \texttt{bond} is an existing node in the walk, it is
removed from it. Finally, the second walk has the particularity to have a node
with a capital letter. In this case, this node is rewritten in lowercase.

\begin{lstlisting}[caption=Splits Nodes of Random Walks with \texttt{SplitWalker}.,label=splitwalker:split]
def basic_split(self, walks):
  canonical_walks = set()
  for walk in walks:
    canonical_walk = [walk[0].name]
    for i, _ in enumerate(walk[1::], 1):
      vertices = []
      if "http" in walk[i].name:
        vertices = " ".join(re.split("[\#]", walk[i].name)).split()
      if i % 2 == 1:
        name = vertices[1] if vertices else walk[i].name
        preds = [
          sub_name
          for sub_name in re.split(r"([A-Z][a-z]*)", name)
          if sub_name
        ]
        for pred in preds:
          canonical_walk += [pred.lower()]
      else:
        name = vertices[-1] if vertices else walk[i].name
        objs = []
        try:
          objs = [str(float(name))]
        except ValueError:
          objs = re.sub("[^A-Za-z0-9]+", " ", name).split()
          if len(objs) == 1:
            match = re.match(
              r"([a-z]+)([0-9]+)", objs[0], re.I
            )
            if match:
              objs = list(match.groups())
        for obj in objs:
          canonical_walk += [obj.lower()]
    canonical_walk = list(dict(zip(canonical_walk, canonical_walk)))
    canonical_walks.add(tuple(canonical_walk))
  return canonical_walks
\end{lstlisting}

Algorithm \ref{splitwalker:split} starts by traversing each provided
walk, making sure to save the root node characterized by the first vertex of the
walk. Then, this function looks to see if that node has the prefix ``http'' for
each node. If it does, then that node is split by the \texttt{\#}
symbol. Otherwise, if the current node is a predicate, this function does an
uppercase split to create other nodes. Finally, this process is also done in the
case where the node contains numbers. In the end, this function deletes all the
duplicated nodes and returns the walks.

\section{WideSampler}
\label{sec:wide:sampler}

Based on the principle that humans tend to classify objects according to common
features (e.g., color and shape), this Master's thesis proposes a new sampling
strategy called \texttt{WideSampler}. This strategy addresses the assumption
that single entity-specific features would have a negligible impact on the
quality of the generated embeddings. \texttt{WideSampler} assigns higher weights
to edges that lead to the most significant number of predicates and objects in
the neighborhood and terms of occurrence in a graph to extract a maximum of
shared features between entities.

After training the model, this walking strategy can retrieve the weight of a
neighboring hop as shown below.
\begin{algorithm}
  \caption{\texttt{get\_weight(h, d, c)}}
  \label{alg:wide:sampler:get:weight}
  \begin{algorithmic}[1]
    \REQUIRE a $\mathcal{H}$ 2-tuple that contains a predicate and an object.
    \REQUIRE a $\mathcal{D}$ array of $n \geq 1$ degree indexed from $0$ to $n - 1$
    \REQUIRE a $\mathcal{C}$ array of $n \geq 1$ counter of neighbors indexed from $0$ to $n - 1$
    \ENSURE The weight of the hop for this predicate
    \IF{$\mathcal{D}_{preds}$ and $\mathcal{D}_{objs}$ and $\mathcal{C}$}
    \RETURN $\left(\mathcal{C}[\mathcal{H}[0]_{name}]\ +\ \mathcal{C}[\mathcal{H}[1]_{name}]\right)$ $\left(\dfrac{\mathcal{D}_{preds}[\mathcal{H}[0]_{name}] + \mathcal{D}_{objs}[\mathcal{H}[1]_{name}]}{2}\right)$
    \ENDIF
  \end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:wide:sampler:get:weight} assigns a weight to a
\texttt{(predicate, object)} hop according to the multiplication of two
sums. The first one considers the child nodes of a predicate and the parent
nodes of an object node. The second one considers the number of occurrences of
this predicate and this node in the whole KG. Finally, the second sum is divided
by half to provide a slight preference to a hop that reaches multiples nodes.

\newpage

\begin{algorithm}
  \caption{\texttt{fit(v)}}
  \label{alg:wide:sampler:fit}
  \begin{algorithmic}[1]
    \REQUIRE an $\mathcal{V}$ array of $n \geq 1$ edges indexed from $0$ to $n - 1$
    \ENSURE Fits the WideSampler sampling strategy
    \STATE $\mathcal{C}_{objs}$ $\leftarrow$ new array of $n$ objects.
    \STATE $\mathcal{C}_{preds}$ $\leftarrow$ new array of $n$ predicates.
    \FORALL{$vertex \in \mathcal{V}$}
    \IF{$vertex$ is predicate}
    \STATE $\mathcal{C}_{neighbors}[vertex_{name}] = |\texttt{get\_children(vertex)}|$
    \STATE $\mathcal{C}_{tmp} \leftarrow \mathcal{C}_{preds}$
    \ELSE
    \STATE $\mathcal{C}_{neighbors}[vertex_{name}] = |\texttt{get\_parents(vertex)}|$
    \STATE $\mathcal{C}_{tmp} \leftarrow \mathcal{C}_{objs}$
    \ENDIF

    \IF{$vertex_{name}$ in $\mathcal{C}_{tmp}$}
    \STATE $\mathcal{C}_{tmp}[vertex_{name}] \leftarrow \mathcal{C}[vertex_{name}] + 1$
    \ELSE
    \STATE $\mathcal{C}_{tmp}[vertex_{name}] \leftarrow 1$
    \ENDIF
    \ENDFOR
  \end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:wide:sampler:fit} trains the \texttt{WideSampler} strategy by
iterating through a set of vertices. Depending on the vertex, the algorithm
considers the number of children or parents of this vertex. If it is a
predicate, then the number of its child nodes is stored in a counter. Otherwise,
the algorithm stores the number of parent nodes in another counter. Finally, the
number of occurrences of each identical vertex in the whole KG is also
collected.

\section{Library Architecture}
\label{sec:pyRDF2Vec}

\texttt{pyRDF2Vec}\footnote{\url{https://github.com/IBCNServices/pyRDF2Vec}} is
the Python library created by IDLab that allows to use RDF2Vec. Through the
internship and this Master's thesis, this library has undergone many changes in
its architecture to improve its use.
\begin{figure}[!ht]
  \centering
  \resizebox{\linewidth}{!} {
    \begin{tikzpicture}[>=stealth',
      extract_walks/.style={draw,minimum width=.8cm,minimum height=.8cm,fill=mygreen!40},
      process/.style={draw,minimum width=1cm,minimum height=1cm,node distance=0.35cm,fill=mybrown!20}
      ]
    \node[draw,minimum width=2.5cm,minimum height=3cm,loosely dashed,color=darkBlue] at (0,0) (graph_entities) {};
    \node[draw,minimum width=2cm,minimum height=1cm,yshift=-65pt,above=of graph_entities,fill=myblue] (graph) {Graph};
    \node[draw,minimum width=2cm,minimum height=1cm,yshift=15pt,below=of graph,fill=myblue] (entities) {Entities};
    \node[draw,minimum width=2cm,minimum height=1cm,yshift=2pt,above=of graph,fill=myblue!50] (connector) {Connector};

    \node[inner sep=0pt,xshift=-20pt,above=of connector] (rdf) {\includegraphics[width=0.074\textwidth]{img/rdf}};
    \node[inner sep=0pt,xshift=20pt,above=of connector] (sparql) {\includegraphics[width=0.08\textwidth]{img/sparql}};

    \node[label,below of=graph_entities,yshift=-1.1cm,color=darkBlue] {\textbf{Inputs}};

    \node[draw,minimum width=3cm,minimum height=1cm,right=of graph_entities,fill=myyellow] (transformer) {Transformer};
    \node[draw,minimum width=2.5cm,minimum height=3cm,loosely dashed,color=darkRed,right=of transformer] (walker_sampler) {};
    \node[draw,minimum width=2cm,minimum height=1cm,yshift=-65pt,above=of walker_sampler,fill=myred] (walker) {Walker};
    \node[draw,minimum width=2cm,minimum height=1cm,yshift=15pt,below=of walker,fill=myred] (sampler) {Sampler};
    \node[draw,minimum width=3cm,minimum height=1cm,above=of transformer,fill=mypurple!50] (embedder) {Embedder};
    \node[draw,ellipse,minimum width=3cm,minimum height=1cm,above=of embedder,fill=mypurple] (embeddings) {Embeddings};
    \node[draw,ellipse,minimum width=3cm,minimum height=1cm,node distance=0.5cm,right=of embeddings,fill=mypurple] (literals) {Literals};

    \node[label,above of=walker_sampler,xshift=0.3cm,yshift=20pt,color=darkRed] {\textbf{Strategy}};

    \node[inner sep=0pt,xshift=-50pt,above right=of embeddings] (sklearn) {\includegraphics[width=0.15\textwidth]{img/scikit-learn}};

    \node[process,node distance=9cm,right=of connector] (p1) {P1/T1};

    \node[process,below=of p1] (p2) {P2/T2};
    \node[process,below=of p2] (p3) {P3/T3};
    \node[process,below=of p3] (p4) {P4/T4};

    \node[extract_walks,right=of p1] (extract_walks_1) {Extract Walks};
    \node[extract_walks,right=of p2] (extract_walks_2) {Extract Walks};
    \node[extract_walks,right=of p3] (extract_walks_3) {Extract Walks};
    \node[extract_walks,right=of p4] (extract_walks_4) {Extract Walks};

    \node[draw,minimum width=7.5cm,minimum height=1.5cm,xshift=1.8cm,yshift=-2.3cm,above=of embeddings,loosely dashed,color=darkPurple] (output) {};
    \node[label,above of=output,xshift=2.5cm,yshift=-0.1cm,color=darkPurple] {\textbf{Outputs}};

    \node[label,below of=p4,yshift=-0.10cm] {\dots};
    \node[label,below of=extract_walks_4,yshift=-0.10cm] {\dots};

    \node[draw,ellipse,minimum width=3cm,minimum height=1cm, node distance=8cm,right=of walker,fill=mygreen] (walks) {Walks};

    \draw[arrow,shorten >=0.2cm,shorten <=0.2cm] (transformer) -- (walker_sampler) node[midway,above] {(2)};

    \draw[arrow,shorten >=0.2cm,shorten <=0.2cm] (rdf.south) -- ([xshift=-20pt]connector.north);
    \draw[arrow,shorten >=0.2cm,shorten <=0.2cm] (sparql.south) -- ([xshift=20pt]connector.north);
    \draw[arrow,shorten >=0.05cm,shorten <=0.2cm] (connector) -- (graph);

    \draw[arrow,shorten >=0.2cm,shorten <=0.2cm] (p1) -- (extract_walks_1);
    \draw[arrow,shorten >=0.2cm,shorten <=0.2cm] (p2) -- (extract_walks_2);
    \draw[arrow,shorten >=0.2cm,shorten <=0.2cm] (p3) -- (extract_walks_3);
    \draw[arrow,shorten >=0.2cm,shorten <=0.2cm] (p4) -- (extract_walks_4);

    \draw[arrow,shorten >=0.2cm,shorten <=0.2cm] (literals) -- (sklearn);

    \draw[shorten <=0.1cm] ([yshift=12pt]walker.east) -| ([xshift=-20pt]p1.west);
    \draw[arrow,shorten >=0.1cm] ([xshift=-20pt]p1.west) -- (p1.west);

    \draw[shorten <=0.1cm] ([yshift=4pt]walker.east) -| ([xshift=-15pt]p2.west);
    \draw[arrow,shorten >=0.1cm] ([xshift=-15pt]p2.west) -- (p2.west);

    \draw[shorten <=0.1cm] ([yshift=-4pt]walker.east) -| ([xshift=-15pt]p3.west);
    \draw[arrow,shorten >=0.1cm] ([xshift=-15pt]p3.west) -- (p3.west);

    \draw[shorten <=0.1cm] ([yshift=-12pt]walker.east) -| ([xshift=-20pt]p4.west);
    \draw[arrow,shorten >=0.1cm] ([xshift=-20pt]p4.west) -- (p4.west);

    \draw[shorten <=0.1cm] (extract_walks_1.east) -| ([xshift=-15pt,yshift=12pt]walks.west);
    \draw[arrow,shorten >=0.1cm] ([xshift=-15pt,yshift=12pt]walks.west) -- ([yshift=12pt]walks.west);

    \draw[shorten <=0.1cm] (extract_walks_2.east) -| ([xshift=-20pt,yshift=4pt]walks.west);
    \draw[arrow,shorten >=0.1cm] ([xshift=-20pt,yshift=4pt]walks.west) -- ([yshift=4pt]walks.west);

    \draw[shorten <=0.1cm] (extract_walks_3.east) -| ([xshift=-20pt,yshift=-4pt]walks.west);
    \draw[arrow,shorten >=0.1cm] ([xshift=-20pt,yshift=-4pt]walks.west) -- ([yshift=-4pt]walks.west);

    \draw[shorten <=0.1cm] (extract_walks_4.east) -| ([xshift=-15pt,yshift=-12pt]walks.west);
    \draw[arrow,shorten >=0.1cm] ([xshift=-15pt,yshift=-12pt]walks.west) -- ([yshift=-12pt]walks.west);

    \draw[arrow,shorten >=0.1cm,shorten <=0.1cm] (sampler) -- (walker);
    \draw[arrow,shorten >=0.2cm,shorten <=0.2cm] (transformer) -- (embedder) node[midway,right] {(3)};
    \draw[arrow,shorten >=0.05cm,shorten <=0.2cm] (embedder) -- (embeddings);
    \draw[arrow,shorten >=0.2cm,shorten <=0.2cm] (embeddings) -- (sklearn);

    \draw[arrow,shorten >=0.2cm,shorten <=0.2cm] (graph_entities) -- (transformer.west) node[midway,above] {(1)};

    \draw[shorten <=0.2cm] ([xshift=0.2cm]transformer) -- ([yshift=1.3cm]walker_sampler.north);
    \draw[arrow,shorten >=0.05cm] ([yshift=1.3cm]walker_sampler.north) -- (literals);

    \draw ([xshift=10pt]walks.east) |- ([yshift=-20pt]extract_walks_4.south);
    \draw[shorten <=0.1cm] (walks) -- ([xshift=10pt]walks.east);
    \draw[arrow,shorten >=0.2cm] ([yshift=-20pt]extract_walks_4.south) -| (transformer.south);
  \end{tikzpicture}
  }
  \caption{Workflow of \texttt{pyRDF2Vec}.}
  \label{tikz:workflow}
\end{figure}

In Figure \ref{tikz:workflow}, the workflow of \texttt{pyRDF2Vec} includes three
primary operations and is divided into seven main consecutive significant
blocks:
\begin{multicols}{2}
\begin{enumerate}
\item \textbf{Connector}: in charge of interaction with a local or remote graph.
\item \textbf{Graph}: in charge of providing a graph encoding the knowledge-based.
\item \textbf{Entities}: in charge of provides the entities in a
\texttt{rdflib.URI.term} or \texttt{str} type to generate the embeddings.
\item \textbf{Transformer}: in charge of converting graphs into embeddings for
downstream ML tasks, using a walking strategy and sampling strategy and an
embedder.
\item \textbf{Sampler}: in charge of prioritizing the use of some paths over
others using a weight allocation strategy.
\item \textbf{Walker}: in charge of extracting walks in a KG from provided
entities and optionally from a sampling strategy using multiple
processors/threads.
\item \textbf{Embedder}: in charge of training a model with an embedding
technique using extracted walks and therefore generate embeddings of entities
provided by a user.
\end{enumerate}
\end{multicols}

The design of such architecture allows having a long-term vision. Due to this
architecture, a user can contribute to \texttt{pyRDF2Vec} and easily add new
walking strategies and sampling and embedding techniques. For this Master’s
thesis, implementing this architecture was necessary to facilitate the
comparison of embedding techniques. Each new embedding technique is added in an
\texttt{Embedder} package and must reimplement the \texttt{fit} and
\texttt{get\_weight} functions of the \texttt{Embedder} class.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "latex"
%%% End:
