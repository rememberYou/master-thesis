\begin{abstract}
\noindent Over the past decade, various use cases have highlighted the benefits
of converting a Knowledge Graph into a 2D feature matrix, called embedding
matrix. This conversion can be done with RDF2Vec, an unsupervised task-agnostic
algorithm for numerically representing Knowledge Graph nodes to be used for
downstream Machine Learning tasks. Since 2016, this algorithm has provided good
results using Word2Vec, an embedding technique initially used in the Natural
Language Processing field. However, other techniques in this field have emerged,
such as BERT, which since 2018, is the state-of-the-art algorithm. The goal of
this Master's thesis mainly focused on evaluating BERT for Knowledge Graphs to
determine its impact compared to Word2Vec and FastText. As a result, this
Master's thesis proposed an implementation of BERT and FastText related to
Knowledge Graphs and improving the node embedding's quality generated by
Word2Vec. For the latter, it was suggested to both extract the root nodes'
parents and centralize the position of these roots within their walk
extraction. This Master's thesis also extended the use of RDF2Vec by introducing
\texttt{SplitWalker} and \texttt{WideSampler} as new walking and sampling
strategies. The study done reveals that the results obtained by BERT with
RDF2Vec are not conclusive, contrary to the expectations. The main reason is the
lack of optimization of the BERT's architecture towards Knowledge Graphs, which
explains the creation of BERT variants in this direction. Finally, the model's
accuracy generated by Word2Vec has increased considerably, and both
\texttt{SplitWalker} and \texttt{WideSampler} have proven their effectiveness in
certain use cases.

\keywords{BERT, Knowledge Graph, Machine Learning, RDF2Vec, Word2Vec}
\end{abstract}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../report"
%%% End:
