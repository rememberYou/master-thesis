
\section{FastText}
\label{sec:fasttext}

FastText is an extension to Word2Vec created by Facebook in 2016, based on the
decomposition of a word into character $n$-grams to improve the embeddings
obtained by an SG model. Unlike Word2Vec, which treats each word in a corpus as
an atomic entity, the decomposition made by FastText mainly solves the
embeddings creation for OOV words and parameter sharing between words of the
same radical. Finally, FastText can be used for unsupervised learning and
supervised learning with text classification for tasks such as spam filtering.

\subsection{Sub-Word Generation}
\label{subsec:fasttext:sub-word}

The generation of sub-words called $n$-grams is an integral part of the success
of FastText. Therefore, it is good to know the details of such a generation.

\begin{definition}[Sub-word Generation]
  Consists of adding angular brackets on either side of a word used as a
  delimiter and generating character $n$-grams of length $n$. In practice, picking
  this length allows extraction of $n$-grams $\geq$ 3 and $\geq
  6$~\citep{DBLP:journals/tacl/BojanowskiGJM17}. Let ``flying'' be a word. The
  following table illustrates the sub-word generation for character $n$-grams of
  length 3, 4, 5, and 6.

  \begin{table}[!ht]
    \centering
    \caption{Sub-word Generation for Character $N$-Grams of Length 3, 4, 5, and 6.}
    \label{tab:fasttext:sub-word}
    \begin{tabular}{ccl}
      \toprule
      \textbf{Word} & \textbf{Length} & \textbf{Character $n$-grams} \\
      \midrule
      \multirow{4}{*}{flying} & 3 & <fl, fly, lyi, yin, ing, ng> \\
                    & 4 & <fly, flyi, lyin, ying, ing> \\
                    & 5 & <flyi, lying, ying> \\
                    & 6 & <flyin, flying, lying> \\
      \bottomrule
    \end{tabular}
  \end{table}

  In Table \ref{tab:fasttext:sub-word}, each character $n$-grams of length $n$
  is generated by sliding a 2-characters window from the beginning of the bracket
  to its end.
  \label{def:sub-word:generation}
\end{definition}

As a result, the vector of a word corresponds to the sum of these $n$-grams of
characters. This word decomposition into character $n$-grams comes at a storage
cost since storing all the unique $n$-grams can quickly be significant. The
original paper uses a variant of the Fowler-Noll-Vo (FNV) hash function, called
\emph{FNV-1a}, to hash character sequences into integer values to reduce this
memory cost. The use of FNV is helpful for this use case since FNV was not
designed for cryptography but for the fast use of hash tables and
\emph{checksums}. The checksum results of performing a cryptographic hash
function on a piece of data, usually a file, to ensure that the data is
authentic and error-free. Moreover, it is necessary to learn a certain amount of
embeddings to hash character sequences. Specifically, this amount designates the
hash bucket's size to better distribute these character $n$-grams for sorting
and retrieval purposes. From then on, the hashing of each $n$-gram to a number
between one and $N$, reduces the vocabulary size in the counterpart of potential
\emph{collisions}. This hash collision means that several character $n-grams$
stored as a key can result in the same hash and checksum. Therefore no longer
ensure the authenticity of a value.

In some circumstances, the model size may be excessive. In this case, it is
still possible to reduce the hash size where the appropriate value is near
$20000$. However, it is also possible to reduce the size of the vectors at the
expense of smaller model accuracy.

\subsection{Training}
\label{subsec:fasttext:training}

Like Word2Vec, the training of FastTextâ€™s SG model can use HSM or Negative
sampling. To better understand its application with negative sampling, one can
take the following ``I always remember her'' sentence is taken. Let ``remember''
be the target word, using 3-grams and a window size of 3. This example predicts
the ``always'' and ``her'' context words. First of all, this model computes the
embeddings of the target word by summing the embeddings for the character
$n$-grams and the whole word itself:
\begin{align}
  \mathrm{E}_{remember} = e_{<re} + e_{rem} + \dots + e_{er>} + e_{remember}
  \label{eq:fasttext:embeddings}
\end{align}

Afterward, each context word is taken directly from their embeddings without
adding the character $n$-grams. Then, several random negative samples are selected
with a probability proportional to the square root of the unigram
frequency. From then on, a context word is related to five random negative
words. In the absence of $n$-gram embeddings, the Word2Vec model is used,
specifying a maximum $n$-gram length of zero.

After selecting samples, the dot product between the target word and the actual
context words computes the probability distribution. Unlike Word2Vec, there is
this time use of the sigmoid function instead of the softmax function. Finally,
the embeddings are updated with an SGD optimizer according to the calculated
loss to bring the actual context words closer to the target word. Afterward,
there is an incrementation in the distance of the negative samples.

This update of hyper-parameters for embeddings of $n$-grams adds a significant
amount of extra computation to the training process. Moreover, CBOW generates
the word embeddings by summing and averaging $n$-grams embeddings, which adds cost
compared to SG. However, these additional calculations benefit from a set of
embeddings containing the sub-words embeddings. From then on, they allow a
better accuracy of a model in most cases.

\subsection{Advantages and Disadvantages}
\label{subsec:fasttext:pro:cons}

In a non-exhaustive way, the advantages of the FastText:
\begin{multicols}{2}
\begin{itemize}
\item The use of unsupervised learning and supervised learning with text
classification for tasks such as spam filtering.
\item Captures the meaning of suffixes/prefixes for the words given in a corpus.
\item Generation of better word embeddings for rare words as their character
$n$-grams are shared with other more frequent words, adding more neighboring
contextual words and improving its probability of being selected.
\columnbreak
  \item Generation of word embeddings for OOV words.
\item Better semantic similarity between two words (e.g., king and kingdom) by
using extra information about the sub-words.
\item No compromise of accuracy when stopwords are present.
\item Significant improvement in model accuracy when used to perform syntactic
word analogy tasks for morphologically rich languages (e.g., French and German).
\end{itemize}
\end{multicols}

As for its main disadvantages, they are the following:
\begin{multicols}{2}
\begin{itemize}
\item Requires much more RAM than Word2Vec due to the sub-words generation for
each word.
\item Lower accuracy compared to Word2Vec when a model is used on semantic
analogy tasks.
\item FastText is \SI{50}{\percent} slower to train than the regular Skip-Gram model due
to the added overhead of $n$-grams compared to Word2Vec.
\item Difficulty in determining the best value for the $n$-grams generation.
\end{itemize}
\end{multicols}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../report"
%%% End:
